{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankbz.desktop-9m3blv3\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\wheel\\pep425tags.py:75: RuntimeWarning: Config variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect\n",
      "  warn=(impl == 'cp')):\n",
      "c:\\users\\ankbz.desktop-9m3blv3\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\wheel\\pep425tags.py:79: RuntimeWarning: Config variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect\n",
      "  warn=(impl == 'cp')):\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading packages\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, TensorDataset, DataLoader\n",
    "from tqdm import tqdm \n",
    "import codecs\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Fix the seeds to get consistent results\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home youâ€™re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52415</td>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45157</td>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesnâ€™t need another CUCK! We alr...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>82776</td>\n",
       "      <td>@USER @USER @USER Itâ€™s not my fault you suppor...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42992</td>\n",
       "      <td>@USER Whatâ€™s the difference between #Kavanaugh...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>28414</td>\n",
       "      <td>@USER you are a lying corrupt traitor!!! Nobod...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54920</td>\n",
       "      <td>@USER @USER @USER It should scare every Americ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56392</td>\n",
       "      <td>@USER @USER @USER @USER @USER @USER @USER @USE...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>86735</td>\n",
       "      <td>@USER you are also the king of taste</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>95686</td>\n",
       "      <td>#MAGA @USER  ðŸŽ¶ Sing like no one is listening  ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>71446</td>\n",
       "      <td>5/5: @USER The time is right for this House to...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23958</td>\n",
       "      <td>@USER Besides Jaxâ€™s mom and maybe Ope he is ha...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28195</td>\n",
       "      <td>@USER @USER @USER gun control! That is all the...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              tweet subtask_a  \\\n",
       "0   86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1   90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...       OFF   \n",
       "2   16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3   62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4   43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "5   97670                  @USER Liberals are all Kookoo !!!       OFF   \n",
       "6   77444                   @USER @USER Oh noes! Tough shit.       OFF   \n",
       "7   52415  @USER was literally just talking about this lo...       OFF   \n",
       "8   45157                         @USER Buy more icecream!!!       NOT   \n",
       "9   13384  @USER Canada doesnâ€™t need another CUCK! We alr...       OFF   \n",
       "10  82776  @USER @USER @USER Itâ€™s not my fault you suppor...       NOT   \n",
       "11  42992  @USER Whatâ€™s the difference between #Kavanaugh...       NOT   \n",
       "12  28414  @USER you are a lying corrupt traitor!!! Nobod...       OFF   \n",
       "13  54920  @USER @USER @USER It should scare every Americ...       NOT   \n",
       "14  56392  @USER @USER @USER @USER @USER @USER @USER @USE...       NOT   \n",
       "15  86735               @USER you are also the king of taste       NOT   \n",
       "16  95686  #MAGA @USER  ðŸŽ¶ Sing like no one is listening  ...       NOT   \n",
       "17  71446  5/5: @USER The time is right for this House to...       NOT   \n",
       "18  23958  @USER Besides Jaxâ€™s mom and maybe Ope he is ha...       NOT   \n",
       "19  28195  @USER @USER @USER gun control! That is all the...       OFF   \n",
       "\n",
       "   subtask_b subtask_c  \n",
       "0        UNT       NaN  \n",
       "1        TIN       IND  \n",
       "2        NaN       NaN  \n",
       "3        UNT       NaN  \n",
       "4        NaN       NaN  \n",
       "5        TIN       OTH  \n",
       "6        UNT       NaN  \n",
       "7        TIN       GRP  \n",
       "8        NaN       NaN  \n",
       "9        TIN       IND  \n",
       "10       NaN       NaN  \n",
       "11       NaN       NaN  \n",
       "12       TIN       IND  \n",
       "13       NaN       NaN  \n",
       "14       NaN       NaN  \n",
       "15       NaN       NaN  \n",
       "16       NaN       NaN  \n",
       "17       NaN       NaN  \n",
       "18       NaN       NaN  \n",
       "19       TIN       OTH  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('dataset/offenseval-training-v1.tsv', sep=\"\\t\", header=0)\n",
    "train_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 8840]\n",
      " [   1 4400]]\n",
      "[[   0 3876]\n",
      " [   1  524]]\n",
      "[[   0 1074]\n",
      " [   1 2407]\n",
      " [   2  395]]\n",
      "train_sents_a shape:  (13240,)\n",
      "train_label_a shape:  (13240,)\n",
      "train_sents_b shape:  (7752,)\n",
      "train_label_b shape:  (7752,)\n",
      "train_sents_c shape:  (7221,)\n",
      "train_label_c shape:  (7221,)\n",
      "[[   0 8840]\n",
      " [   1 4400]]\n",
      "[[   0 3876]\n",
      " [   1 3876]]\n",
      "[[   0 2407]\n",
      " [   1 2407]\n",
      " [   2 2407]]\n"
     ]
    }
   ],
   "source": [
    "#Load dataset\n",
    "train_data = pd.read_csv('dataset/offenseval-training-v1.tsv', sep=\"\\t\", header=0).values\n",
    "train_sents_a = np.array(train_data[:,1].astype(str))\n",
    "\n",
    "# Integer labeling\n",
    "# NOT : 0, OFF : 1\n",
    "_, train_label_a = np.unique(train_data[:,2].astype(str), return_inverse=True)\n",
    "#TIN : 0, UNT : 1, nan: 2\n",
    "_, train_label_b = np.unique(train_data[:,3].astype(str), return_inverse=True)\n",
    "#GRP: 0, IND : 1, OTH: 2, nan: 3\n",
    "_, train_label_c = np.unique(train_data[:,4].astype(str), return_inverse=True)\n",
    "\n",
    "#TIN : 0, UNT : 1\n",
    "train_sents_b = np.delete(train_sents_a, np.nonzero(train_label_b == 2), axis=0)\n",
    "train_label_b = np.delete(train_label_b, np.nonzero(train_label_b == 2), axis=0)\n",
    "\n",
    "#GRP: 0, IND : 1, OTH: 2\n",
    "train_sents_c = np.delete(train_sents_a, np.nonzero(train_label_c == 3), axis=0)\n",
    "train_label_c = np.delete(train_label_c, np.nonzero(train_label_c == 3), axis=0)\n",
    "\n",
    "# Print out the samples for each class labels\n",
    "unique, counts = np.unique(train_label_a, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "unique, counts = np.unique(train_label_b, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "unique, counts = np.unique(train_label_c, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "# Limited by the number of data, we oversample rather than downsample the data to make sure the classes in task B and C are balanced.\n",
    "rus = RandomOverSampler(random_state=0)\n",
    "train_sents_b, train_label_b = rus.fit_resample(train_sents_b.reshape(-1, 1), train_label_b)\n",
    "train_sents_c, train_label_c = rus.fit_resample(train_sents_c.reshape(-1, 1), train_label_c)\n",
    "\n",
    "train_sents_b = train_sents_b.reshape(-1, )\n",
    "train_sents_c = train_sents_c.reshape(-1, )\n",
    "\n",
    "print('train_sents_a shape: ', train_sents_a.shape)\n",
    "print('train_label_a shape: ', train_label_a.shape)\n",
    "print('train_sents_b shape: ', train_sents_b.shape)\n",
    "print('train_label_b shape: ', train_label_b.shape)\n",
    "print('train_sents_c shape: ', train_sents_c.shape)\n",
    "print('train_label_c shape: ', train_label_c.shape)\n",
    "\n",
    "# Print out the samples for each class labels\n",
    "unique, counts = np.unique(train_label_a, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "unique, counts = np.unique(train_label_b, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "unique, counts = np.unique(train_label_c, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test set for submission\n",
    "def getEvaTensor(test_data, word2idx, max_len):\n",
    "    place_hold = torch.empty((1,1))\n",
    "    tokenized_corpus = get_tokenized_corpus(test_data)\n",
    "    test_sent_tensor, _ = get_model_inputs(tokenized_corpus, word2idx, place_hold, max_len)\n",
    "    \n",
    "    return test_sent_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eva_data_a shape:  torch.Size([860, 105])\n",
      "eva_data_b shape:  torch.Size([240, 103])\n",
      "eva_data_c shape:  torch.Size([213, 103])\n",
      "eva id a shape:  (860,)\n",
      "eva id b shape:  (240,)\n",
      "eva id c shape:  (213,)\n"
     ]
    }
   ],
   "source": [
    "# For test set for submission\n",
    "eva_data_a = getEvaTensor(np.array(pd.read_csv('dataset/testset-taska.tsv', sep=\"\\t\", header=0).astype(str))[:, 1], word2idx_a, max_len_a)\n",
    "eva_data_b = getEvaTensor(np.array(pd.read_csv('dataset/testset-taskb.tsv', sep=\"\\t\", header=0).astype(str))[:, 1], word2idx_b, max_len_b)\n",
    "eva_data_c = getEvaTensor(np.array(pd.read_csv('dataset/test_set_taskc.tsv', sep=\"\\t\", header=0).astype(str))[:, 1], word2idx_c, max_len_c)\n",
    "\n",
    "eva_id_a = np.array(pd.read_csv('dataset/testset-taska.tsv', sep=\"\\t\", header=0).astype(str))[:, 0]\n",
    "eva_id_b = np.array(pd.read_csv('dataset/testset-taskb.tsv', sep=\"\\t\", header=0).astype(str))[:, 0]\n",
    "eva_id_c = np.array(pd.read_csv('dataset/test_set_taskc.tsv', sep=\"\\t\", header=0).astype(str))[:, 0]\n",
    "\n",
    "print('eva_data_a shape: ', eva_data_a.shape)\n",
    "print('eva_data_b shape: ', eva_data_b.shape)\n",
    "print('eva_data_c shape: ', eva_data_c.shape)\n",
    "\n",
    "print('eva id a shape: ', eva_id_a.shape)\n",
    "print('eva id b shape: ', eva_id_b.shape)\n",
    "print('eva id c shape: ', eva_id_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "    \n",
    "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
    "    for sentence in corpus:\n",
    "        sentence = re.sub(r'([A-Z])', lambda pat: pat.group(1).lower(), sentence)\n",
    "        sentence = re.findall(r'\\w+', sentence)\n",
    "        tokenized_corpus.append(sentence)\n",
    " \n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2idx(tokenized_corpus):\n",
    "    vocabulary = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.append(token)\n",
    "  \n",
    "    word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
    "    # we reserve the 0 index for the placeholder token\n",
    "    word2idx['<pad>'] = 0\n",
    " \n",
    "    return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len):\n",
    "\n",
    "    # we index our sentences\n",
    "    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
    " \n",
    "    # we create a tensor of a fixed size filled with zeroes for padding\n",
    "    sent_tensor = Variable(torch.zeros((len(vectorized_sents), max_len))).long()  \n",
    "    sent_lengths = [len(sent) for sent in vectorized_sents]\n",
    "  \n",
    "    # we fill it with our vectorized sentences \n",
    "    for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
    "        sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
    "    \n",
    "    label_tensor = torch.FloatTensor(labels)\n",
    "  \n",
    "    return sent_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoaders(sents, label, test_size=0.2):\n",
    "    tokenized_corpus = get_tokenized_corpus(sents)\n",
    "    sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
    "    max_len = np.max(np.array(sent_lengths))    \n",
    "    word2idx = get_word2idx(tokenized_corpus)\n",
    "    sent_tensor, label_tensor = get_model_inputs(tokenized_corpus, word2idx, label, max_len)\n",
    "    \n",
    "    train_idx, valid_idx= train_test_split(np.arange(len(label_tensor)), test_size=test_size, random_state=SEED, shuffle=True, stratify = label)\n",
    "    valid_idx, test_idx= train_test_split(valid_idx, test_size=0.5, random_state=SEED, shuffle=True)\n",
    "    \n",
    "    dataset = TensorDataset(sent_tensor, label_tensor)\n",
    "    train_set = Subset(dataset, train_idx)\n",
    "    valid_set = Subset(dataset, valid_idx)\n",
    "    test_set = Subset(dataset, test_idx)\n",
    " \n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader, max_len, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader for training, validation and test\n",
    "train_a_loader, valid_a_loader, test_a_loader, max_len_a, word2idx_a = getDataLoaders(train_sents_a, train_label_a, 0.4)\n",
    "train_b_loader, valid_b_loader, test_b_loader, max_len_b, word2idx_b = getDataLoaders(train_sents_b, train_label_b, 0.4)\n",
    "train_c_loader, valid_c_loader, test_c_loader, max_len_c, word2idx_c = getDataLoaders(train_sents_c, train_label_c, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7944\n",
      "2648\n",
      "2648\n"
     ]
    }
   ],
   "source": [
    "print(len(train_a_loader.dataset))\n",
    "print(len(valid_a_loader.dataset))\n",
    "print(len(test_a_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(output, gold):\n",
    "    output = output >= 0.5\n",
    "    output = output.cpu().numpy()\n",
    "    gold = gold.cpu().numpy()\n",
    "\n",
    "    recall, precision, fscore, _ = precision_recall_fscore_support(output, gold, average='weighted')\n",
    "    \n",
    "    return recall, precision, fscore\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kBhE6FeRuDu"
   },
   "outputs": [],
   "source": [
    " def accuracy_binary(output, target):\n",
    "    \n",
    "    correct = torch.sum((output >= 0.5).float() == target.float(), dtype = torch.float)\n",
    "    acc = correct / len(target)\n",
    " \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1193518/1193518 [00:23<00:00, 50706.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove.6B.zip\n",
    "\n",
    "wvecs = np.zeros((len(word2idx_a), 100))\n",
    "\n",
    "with codecs.open('glove.twitter.27B.100d.txt', 'r','utf-8') as f: \n",
    "    index = 0\n",
    "    for line in tqdm(f.readlines()):\n",
    "        if len(line.strip().split()) > 3:\n",
    "            word = line.strip().split()[0]\n",
    "        if word in word2idx_a:\n",
    "            (word, vec) = (word, list(map(float,line.strip().split()[1:])))\n",
    "            idx = word2idx_a[word]\n",
    "            wvecs[idx] = vec\n",
    "\n",
    "#model.embedding.weight.data.copy_(torch.from_numpy(wvecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, test_loader):  \n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "\n",
    "        recall = 0\n",
    "        precision = 0\n",
    "        fscore = 0\n",
    "        \n",
    "        count_train = 0\n",
    "        count_valid = 0\n",
    "        count_test = 0\n",
    "        \n",
    "        model.train()\n",
    "       \n",
    "        #iterate over batches\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            #place on the GPU\n",
    "            #feature, target = batch.text.cuda(), batch.label.cuda()\n",
    "            \n",
    "            feature, target = batch\n",
    "            \n",
    "            feature = feature.to('cuda')\n",
    "            target = target.to('cuda')\n",
    " \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(feature).squeeze(1)\n",
    "             \n",
    "            loss = loss_fn(predictions, target)\n",
    "            acc = accuracy_binary(predictions, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            count_train += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_valid in valid_loader:\n",
    "                feature_valid, target_valid = batch_valid\n",
    "                \n",
    "                feature_valid = feature_valid.to('cuda')\n",
    "                target_valid = target_valid.to('cuda')\n",
    "            \n",
    "                predictions_valid = model(feature_valid).squeeze(1)\n",
    "                loss = loss_fn(predictions_valid, target_valid)\n",
    "                acc = accuracy_binary(predictions_valid, target_valid)\n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += acc.item()\n",
    "                count_valid += 1\n",
    "            \n",
    "        valid_loss = valid_loss /count_valid\n",
    "        valid_acc = valid_acc /count_valid\n",
    "        epoch_loss = epoch_loss /count_train\n",
    "        epoch_acc = epoch_acc /count_train\n",
    "        \n",
    "        print('Epoch: {}. Train Loss: {:.3f}. Train Accuracy: {:.3f}  Val. Loss: {:.3f} Val. Acc: {:.3f}'.format(epoch, epoch_loss, epoch_acc*100, valid_loss, valid_acc*100))\n",
    "        \n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_test in test_loader:\n",
    "            feature_test, target_test = batch_test\n",
    "            \n",
    "            feature_test = feature_test.to('cuda')\n",
    "            target_test = target_test.to('cuda')\n",
    "            \n",
    "            predictions_test = model(feature_test).squeeze(1)\n",
    "            loss = loss_fn(predictions_test, target_test)\n",
    "            acc = accuracy_binary(predictions_test, target_test)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += acc.item()\n",
    "            r, p, f = f_measure(predictions_test, target_test)\n",
    "            recall += r\n",
    "            precision += p\n",
    "            fscore += f\n",
    "            count_test += 1\n",
    "\n",
    "    test_loss = test_loss /count_test\n",
    "    test_acc = test_acc /count_test\n",
    "    recall = recall /count_test\n",
    "    precision = precision /count_test\n",
    "    fscore = fscore /count_test\n",
    "    \n",
    "    print('Test Loss: {:.3f}. Test Accuracy: {:.3f}'.format(test_loss, test_acc*100))\n",
    "    print(\"Test: Recall: %.2f, Precision: %.2f, F-measure: %.2f\\n\" % (recall, precision, fscore))\n",
    "    \n",
    "    return test_acc, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #in_channels -- 1 text channel\n",
    "        #out_channels -- the number of output channels\n",
    "        #kernel_size is (window size x embedding dim)\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size,embedding_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_channels, output_dim)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        #(batch size, max sent length)\n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #(batch size, max sent length, embedding dim)\n",
    "        #images have 3 RGB channels \n",
    "        #for the text we add 1 channel\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #(batch size, 1, max sent length, embedding dim)\n",
    "        feature_maps = self.conv(embedded)\n",
    "\n",
    "        #(batch size, out_channels, max sent length - window_size + 1, 1)\n",
    "        feature_maps = feature_maps.squeeze(3)\n",
    "        \n",
    "        #remove last 1 dim\n",
    "        feature_maps = F.relu(feature_maps)\n",
    "        \n",
    "        #the max pooling layer\n",
    "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
    "        pooled = pooled.squeeze(2)\n",
    "  \n",
    "        #(batch size, out_channels, 1)\n",
    "        dropped = self.dropout(pooled)\n",
    "        \n",
    "        #(batch size, out_channels)\n",
    "        preds = self.fc(dropped)\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, out_channels):\n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.n_hidden = 5\n",
    "        self.input_size = 8\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.input_size, kernel_size=(1,embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=self.input_size, kernel_size=(3,embedding_dim))\n",
    "        self.conv5 = nn.Conv2d(in_channels=1, out_channels=self.input_size, kernel_size=(5,embedding_dim))\n",
    "        \n",
    "        self.gru1 = nn.GRU(self.input_size, self.n_hidden)\n",
    "        self.gru3 = nn.GRU(self.input_size, self.n_hidden)\n",
    "        self.gru5 = nn.GRU(self.input_size, self.n_hidden)\n",
    "        \n",
    "        self.fc = nn.Linear(3*self.n_hidden, out_channels)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, _ = x.shape\n",
    "                \n",
    "        #(batch size, max sent length)\n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #(batch size, max sent length, embedding dim)\n",
    "        #for the text we add 1 channel\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #(batch size, 1, max sent length, embedding dim)\n",
    "        \n",
    "        f1 = self.conv1(embedded)\n",
    "        #(batch size, out_channels, max sent length - window_size + 1, 1)\n",
    "        f1 = f1.squeeze(3)\n",
    "        f1 = F.relu(f1)\n",
    "        #(batch size, out_channels, max sent length - window_size + 1)\n",
    "        f1 = torch.transpose(torch.transpose(f1, 1, 2), 0, 1)\n",
    "        #(max sent length - window_size + 1, batch size, out_channels)\n",
    "        self.h1 = self.init_hidden(batch_size)\n",
    "        gru_out1, self.h1 = self.gru1(f1, self.h1)\n",
    "        \n",
    "        f3 = self.conv3(embedded)\n",
    "        #(batch size, out_channels, max sent length - window_size + 1, 1)\n",
    "        f3 = f3.squeeze(3)\n",
    "        f3 = F.relu(f3)\n",
    "        #(batch size, out_channels, max sent length - window_size + 1)\n",
    "        f3 = torch.transpose(torch.transpose(f3, 1, 2), 0, 1)\n",
    "        #(max sent length - window_size + 1, batch size, out_channels)\n",
    "        self.h3 = self.init_hidden(batch_size)\n",
    "        gru_out3, self.h3 = self.gru3(f3, self.h3)\n",
    "        \n",
    "        \n",
    "        f5 = self.conv5(embedded)\n",
    "        #(batch size, out_channels, max sent length - window_size + 1, 1)\n",
    "        f5 = f5.squeeze(3)\n",
    "        f5 = F.relu(f5)\n",
    "        #(batch size, out_channels, max sent length - window_size + 1)\n",
    "        f5 = torch.transpose(torch.transpose(f5, 1, 2), 0, 1)\n",
    "        #(max sent length - window_size + 1, batch size, out_channels)\n",
    "        self.h5 = self.init_hidden(batch_size)\n",
    "        gru_out5, self.h5 = self.gru5(f5, self.h5)\n",
    "        \n",
    "        c = torch.cat((torch.cat((gru_out1[-1], gru_out3[-1]), dim = 1), gru_out5[-1]), dim = 1)\n",
    "        \n",
    "        preds = self.fc(c)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros((1,batch_size,self.n_hidden))).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate factor:  0.4\n",
      "Epoch: 1. Train Loss: 0.642. Train Accuracy: 67.017  Val. Loss: 0.627 Val. Acc: 68.037\n",
      "Epoch: 2. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 3. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.641 Val. Acc: 67.155\n",
      "Epoch: 4. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.901\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 7. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.698\n",
      "Epoch: 8. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.638 Val. Acc: 67.291\n",
      "Epoch: 9. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.627 Val. Acc: 68.104\n",
      "Epoch: 10. Train Loss: 0.630. Train Accuracy: 67.786  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 11. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.626 Val. Acc: 68.172\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 14. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 18. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 19. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankbz.desktop-9m3blv3\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.640. Test Accuracy: 66.142\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.41000000000000003\n",
      "Epoch: 1. Train Loss: 0.637. Train Accuracy: 67.017  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Epoch: 2. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.628 Val. Acc: 68.037\n",
      "Epoch: 3. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.636 Val. Acc: 67.087\n",
      "Epoch: 4. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 5. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 8. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 10. Train Loss: 0.643. Train Accuracy: 65.881  Val. Loss: 0.633 Val. Acc: 67.562\n",
      "Epoch: 11. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 13. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.629 Val. Acc: 68.240\n",
      "Epoch: 14. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 17. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 18. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 19. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Test Loss: 0.643. Test Accuracy: 65.735\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.42000000000000004\n",
      "Epoch: 1. Train Loss: 0.655. Train Accuracy: 65.881  Val. Loss: 0.654 Val. Acc: 67.833\n",
      "Epoch: 2. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.426\n",
      "Epoch: 3. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.833\n",
      "Epoch: 5. Train Loss: 0.639. Train Accuracy: 66.260  Val. Loss: 0.643 Val. Acc: 67.359\n",
      "Epoch: 6. Train Loss: 0.641. Train Accuracy: 66.260  Val. Loss: 0.640 Val. Acc: 67.359\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 8. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.901\n",
      "Epoch: 9. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 10. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 11. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.637 Val. Acc: 67.562\n",
      "Epoch: 12. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 13. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.359\n",
      "Epoch: 14. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 16. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.765\n",
      "Epoch: 17. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 18. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 19. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 20. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.636 Val. Acc: 67.562\n",
      "Test Loss: 0.645. Test Accuracy: 65.803\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.43000000000000005\n",
      "Epoch: 1. Train Loss: 0.649. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.698\n",
      "Epoch: 2. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.630\n",
      "Epoch: 3. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.635 Val. Acc: 67.291\n",
      "Epoch: 4. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 5. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.646 Val. Acc: 67.901\n",
      "Epoch: 6. Train Loss: 0.641. Train Accuracy: 66.260  Val. Loss: 0.635 Val. Acc: 67.426\n",
      "Epoch: 7. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.636 Val. Acc: 67.359\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 68.037\n",
      "Epoch: 9. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.640 Val. Acc: 66.952\n",
      "Epoch: 10. Train Loss: 0.645. Train Accuracy: 65.515  Val. Loss: 0.649 Val. Acc: 67.291\n",
      "Epoch: 11. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 12. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 13. Train Loss: 0.646. Train Accuracy: 65.515  Val. Loss: 0.639 Val. Acc: 67.698\n",
      "Epoch: 14. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.630 Val. Acc: 68.308\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.636 Val. Acc: 66.816\n",
      "Epoch: 16. Train Loss: 0.632. Train Accuracy: 66.650  Val. Loss: 0.612 Val. Acc: 67.426\n",
      "Epoch: 17. Train Loss: 0.542. Train Accuracy: 67.932  Val. Loss: 0.604 Val. Acc: 67.630\n",
      "Epoch: 18. Train Loss: 0.430. Train Accuracy: 78.369  Val. Loss: 0.580 Val. Acc: 72.401\n",
      "Epoch: 19. Train Loss: 0.345. Train Accuracy: 88.257  Val. Loss: 0.542 Val. Acc: 74.729\n",
      "Epoch: 20. Train Loss: 0.283. Train Accuracy: 91.577  Val. Loss: 0.561 Val. Acc: 74.358\n",
      "Test Loss: 0.585. Test Accuracy: 71.604\n",
      "Test: Recall: 0.74, Precision: 0.72, F-measure: 0.73\n",
      "\n",
      "Learning rate factor:  0.44000000000000006\n",
      "Epoch: 1. Train Loss: 0.633. Train Accuracy: 67.395  Val. Loss: 0.632 Val. Acc: 67.901\n",
      "Epoch: 2. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.634 Val. Acc: 67.426\n",
      "Epoch: 3. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.636 Val. Acc: 67.562\n",
      "Epoch: 4. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 5. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 7. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.291\n",
      "Epoch: 8. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 9. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.634 Val. Acc: 67.698\n",
      "Epoch: 10. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 11. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 12. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.765\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 14. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.636 Val. Acc: 66.884\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.698\n",
      "Epoch: 16. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 17. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 18. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 19. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.698\n",
      "Epoch: 20. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.630 Val. Acc: 67.901\n",
      "Test Loss: 0.643. Test Accuracy: 65.735\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.45000000000000007\n",
      "Epoch: 1. Train Loss: 0.644. Train Accuracy: 65.881  Val. Loss: 0.642 Val. Acc: 67.833\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 3. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.631 Val. Acc: 68.104\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 5. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 7. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 10. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.638 Val. Acc: 66.952\n",
      "Epoch: 11. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 12. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.698\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.634 Val. Acc: 67.020\n",
      "Epoch: 14. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 16. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 17. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.291\n",
      "Epoch: 18. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.638 Val. Acc: 67.087\n",
      "Epoch: 19. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Test Loss: 0.636. Test Accuracy: 66.752\n",
      "Test: Recall: 1.00, Precision: 0.67, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.4600000000000001\n",
      "Epoch: 1. Train Loss: 0.646. Train Accuracy: 66.260  Val. Loss: 0.640 Val. Acc: 67.901\n",
      "Epoch: 2. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.640 Val. Acc: 67.426\n",
      "Epoch: 3. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.633 Val. Acc: 67.426\n",
      "Epoch: 5. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.628 Val. Acc: 67.969\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 10. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 11. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 12. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.901\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 14. Train Loss: 0.630. Train Accuracy: 66.650  Val. Loss: 0.593 Val. Acc: 67.969\n",
      "Epoch: 15. Train Loss: 0.554. Train Accuracy: 67.688  Val. Loss: 0.551 Val. Acc: 72.030\n",
      "Epoch: 16. Train Loss: 0.450. Train Accuracy: 76.746  Val. Loss: 0.537 Val. Acc: 74.412\n",
      "Epoch: 17. Train Loss: 0.366. Train Accuracy: 84.851  Val. Loss: 0.586 Val. Acc: 73.689\n",
      "Epoch: 18. Train Loss: 0.310. Train Accuracy: 89.258  Val. Loss: 0.551 Val. Acc: 75.278\n",
      "Epoch: 19. Train Loss: 0.242. Train Accuracy: 92.847  Val. Loss: 0.566 Val. Acc: 75.781\n",
      "Epoch: 20. Train Loss: 0.200. Train Accuracy: 94.885  Val. Loss: 0.611 Val. Acc: 73.557\n",
      "Test Loss: 0.623. Test Accuracy: 72.676\n",
      "Test: Recall: 0.73, Precision: 0.73, F-measure: 0.73\n",
      "\n",
      "Learning rate factor:  0.4700000000000001\n",
      "Epoch: 1. Train Loss: 0.642. Train Accuracy: 67.017  Val. Loss: 0.639 Val. Acc: 67.765\n",
      "Epoch: 2. Train Loss: 0.633. Train Accuracy: 67.395  Val. Loss: 0.652 Val. Acc: 67.630\n",
      "Epoch: 3. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 5. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.636 Val. Acc: 67.291\n",
      "Epoch: 6. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.642 Val. Acc: 67.359\n",
      "Epoch: 7. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.640 Val. Acc: 67.698\n",
      "Epoch: 8. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.643 Val. Acc: 67.291\n",
      "Epoch: 9. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 10. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.833\n",
      "Epoch: 11. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.649 Val. Acc: 67.359\n",
      "Epoch: 12. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 14. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.644 Val. Acc: 66.884\n",
      "Epoch: 15. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 68.037\n",
      "Epoch: 16. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.634 Val. Acc: 67.359\n",
      "Epoch: 17. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.637 Val. Acc: 67.630\n",
      "Epoch: 18. Train Loss: 0.630. Train Accuracy: 67.786  Val. Loss: 0.639 Val. Acc: 67.562\n",
      "Epoch: 19. Train Loss: 0.646. Train Accuracy: 65.515  Val. Loss: 0.637 Val. Acc: 68.172\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Test Loss: 0.644. Test Accuracy: 65.599\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.4800000000000001\n",
      "Epoch: 1. Train Loss: 0.643. Train Accuracy: 66.626  Val. Loss: 0.634 Val. Acc: 67.155\n",
      "Epoch: 2. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.652 Val. Acc: 67.155\n",
      "Epoch: 3. Train Loss: 0.636. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 4. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.833\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.634 Val. Acc: 67.359\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.969\n",
      "Epoch: 8. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.901\n",
      "Epoch: 10. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.223\n",
      "Epoch: 11. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 12. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.901\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 14. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.698\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 16. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.632 Val. Acc: 68.443\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.833\n",
      "Epoch: 18. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 19. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.637 Val. Acc: 67.359\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Test Loss: 0.640. Test Accuracy: 66.209\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.4900000000000001\n",
      "Epoch: 1. Train Loss: 0.641. Train Accuracy: 66.260  Val. Loss: 0.639 Val. Acc: 67.494\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 3. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.634 Val. Acc: 67.426\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.155\n",
      "Epoch: 5. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 7. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 9. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 10. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.629 Val. Acc: 68.037\n",
      "Epoch: 11. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 12. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13. Train Loss: 0.645. Train Accuracy: 65.515  Val. Loss: 0.634 Val. Acc: 67.426\n",
      "Epoch: 14. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.640 Val. Acc: 67.087\n",
      "Epoch: 15. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.629 Val. Acc: 67.969\n",
      "Epoch: 16. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 17. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.637 Val. Acc: 66.884\n",
      "Epoch: 18. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 19. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 20. Train Loss: 0.628. Train Accuracy: 67.786  Val. Loss: 0.642 Val. Acc: 66.681\n",
      "Test Loss: 0.651. Test Accuracy: 65.803\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.5000000000000001\n",
      "Epoch: 1. Train Loss: 0.633. Train Accuracy: 67.395  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 2. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.628 Val. Acc: 67.969\n",
      "Epoch: 3. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 6. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.698\n",
      "Epoch: 7. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.624 Val. Acc: 68.443\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.634 Val. Acc: 67.155\n",
      "Epoch: 9. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 10. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 11. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 12. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 13. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 14. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.291\n",
      "Epoch: 15. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 16. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 17. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 18. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 19. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 20. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.636 Val. Acc: 66.884\n",
      "Test Loss: 0.646. Test Accuracy: 65.735\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.5100000000000001\n",
      "Epoch: 1. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 2. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.634 Val. Acc: 67.020\n",
      "Epoch: 3. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 7. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 8. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 9. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.636 Val. Acc: 67.291\n",
      "Epoch: 10. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 11. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 12. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.650 Val. Acc: 67.087\n",
      "Epoch: 13. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.635 Val. Acc: 66.884\n",
      "Epoch: 14. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 16. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 17. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Epoch: 18. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Epoch: 19. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 20. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Test Loss: 0.641. Test Accuracy: 66.142\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.5200000000000001\n",
      "Epoch: 1. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.634 Val. Acc: 67.359\n",
      "Epoch: 2. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 68.037\n",
      "Epoch: 3. Train Loss: 0.638. Train Accuracy: 66.260  Val. Loss: 0.629 Val. Acc: 67.969\n",
      "Epoch: 4. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.634 Val. Acc: 67.833\n",
      "Epoch: 5. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.637 Val. Acc: 67.291\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 7. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.626 Val. Acc: 68.104\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.634 Val. Acc: 67.020\n",
      "Epoch: 10. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 11. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 12. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.633 Val. Acc: 67.630\n",
      "Epoch: 13. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 14. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.635 Val. Acc: 66.952\n",
      "Epoch: 16. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.627 Val. Acc: 67.969\n",
      "Epoch: 18. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 19. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 20. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.631 Val. Acc: 67.833\n",
      "Test Loss: 0.640. Test Accuracy: 66.209\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.5300000000000001\n",
      "Epoch: 1. Train Loss: 0.637. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 2. Train Loss: 0.633. Train Accuracy: 67.395  Val. Loss: 0.636 Val. Acc: 67.359\n",
      "Epoch: 3. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.901\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 5. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.632 Val. Acc: 67.698\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 7. Train Loss: 0.633. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 8. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.632 Val. Acc: 67.833\n",
      "Epoch: 9. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 10. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.637 Val. Acc: 67.494\n",
      "Epoch: 11. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.636 Val. Acc: 67.291\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 13. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 14. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 15. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.426\n",
      "Epoch: 16. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.635 Val. Acc: 66.952\n",
      "Epoch: 17. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 18. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 67.969\n",
      "Epoch: 19. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 20. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.642 Val. Acc: 67.901\n",
      "Test Loss: 0.650. Test Accuracy: 65.938\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.5400000000000001\n",
      "Epoch: 1. Train Loss: 0.639. Train Accuracy: 66.638  Val. Loss: 0.630 Val. Acc: 67.969\n",
      "Epoch: 2. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 3. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.633 Val. Acc: 68.037\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 5. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.634 Val. Acc: 67.291\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 9. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.562\n",
      "Epoch: 10. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 11. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.640 Val. Acc: 67.494\n",
      "Epoch: 12. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 14. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.631 Val. Acc: 67.765\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 16. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 68.037\n",
      "Epoch: 17. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.636 Val. Acc: 67.698\n",
      "Epoch: 18. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.635 Val. Acc: 67.020\n",
      "Epoch: 19. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.635 Val. Acc: 67.765\n",
      "Epoch: 20. Train Loss: 0.600. Train Accuracy: 66.272  Val. Loss: 0.603 Val. Acc: 67.401\n",
      "Test Loss: 0.613. Test Accuracy: 66.109\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.5500000000000002\n",
      "Epoch: 1. Train Loss: 0.639. Train Accuracy: 66.638  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 2. Train Loss: 0.639. Train Accuracy: 66.260  Val. Loss: 0.635 Val. Acc: 67.765\n",
      "Epoch: 3. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.635 Val. Acc: 66.952\n",
      "Epoch: 5. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 7. Train Loss: 0.631. Train Accuracy: 67.395  Val. Loss: 0.629 Val. Acc: 67.901\n",
      "Epoch: 8. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Epoch: 9. Train Loss: 0.645. Train Accuracy: 65.503  Val. Loss: 0.632 Val. Acc: 68.240\n",
      "Epoch: 10. Train Loss: 0.645. Train Accuracy: 65.503  Val. Loss: 0.635 Val. Acc: 67.426\n",
      "Epoch: 11. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 12. Train Loss: 0.631. Train Accuracy: 67.395  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 13. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 14. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.020\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 16. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.636 Val. Acc: 66.816\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 18. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 19. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.635 Val. Acc: 67.223\n",
      "Epoch: 20. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Test Loss: 0.642. Test Accuracy: 66.074\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.5600000000000002\n",
      "Epoch: 1. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 2. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.635 Val. Acc: 67.359\n",
      "Epoch: 3. Train Loss: 0.648. Train Accuracy: 65.125  Val. Loss: 0.637 Val. Acc: 67.562\n",
      "Epoch: 4. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 5. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.634 Val. Acc: 67.155\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 8. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.426\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 10. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.625 Val. Acc: 68.240\n",
      "Epoch: 11. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 14. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 15. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.359\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.625 Val. Acc: 68.308\n",
      "Epoch: 17. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.635 Val. Acc: 67.833\n",
      "Epoch: 18. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 19. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 20. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.629 Val. Acc: 68.037\n",
      "Test Loss: 0.638. Test Accuracy: 66.548\n",
      "Test: Recall: 1.00, Precision: 0.67, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.5700000000000002\n",
      "Epoch: 1. Train Loss: 0.651. Train Accuracy: 65.564  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 2. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 3. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 4. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.642 Val. Acc: 67.562\n",
      "Epoch: 5. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 7. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.643 Val. Acc: 67.087\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 67.494\n",
      "Epoch: 9. Train Loss: 0.616. Train Accuracy: 66.638  Val. Loss: 0.595 Val. Acc: 67.359\n",
      "Epoch: 10. Train Loss: 0.545. Train Accuracy: 66.650  Val. Loss: 0.575 Val. Acc: 67.698\n",
      "Epoch: 11. Train Loss: 0.445. Train Accuracy: 68.799  Val. Loss: 0.550 Val. Acc: 75.481\n",
      "Epoch: 12. Train Loss: 0.359. Train Accuracy: 87.134  Val. Loss: 0.504 Val. Acc: 76.898\n",
      "Epoch: 13. Train Loss: 0.302. Train Accuracy: 92.065  Val. Loss: 0.515 Val. Acc: 76.788\n",
      "Epoch: 14. Train Loss: 0.264. Train Accuracy: 94.019  Val. Loss: 0.541 Val. Acc: 75.423\n",
      "Epoch: 15. Train Loss: 0.226. Train Accuracy: 94.995  Val. Loss: 0.572 Val. Acc: 75.068\n",
      "Epoch: 16. Train Loss: 0.187. Train Accuracy: 95.544  Val. Loss: 0.615 Val. Acc: 74.028\n",
      "Epoch: 17. Train Loss: 0.160. Train Accuracy: 96.179  Val. Loss: 0.645 Val. Acc: 74.693\n",
      "Epoch: 18. Train Loss: 0.147. Train Accuracy: 96.521  Val. Loss: 0.654 Val. Acc: 74.881\n",
      "Epoch: 19. Train Loss: 0.121. Train Accuracy: 97.339  Val. Loss: 0.672 Val. Acc: 75.200\n",
      "Epoch: 20. Train Loss: 0.114. Train Accuracy: 97.375  Val. Loss: 0.756 Val. Acc: 69.254\n",
      "Test Loss: 0.797. Test Accuracy: 68.217\n",
      "Test: Recall: 0.68, Precision: 0.68, F-measure: 0.68\n",
      "\n",
      "Learning rate factor:  0.5800000000000002\n",
      "Epoch: 1. Train Loss: 0.634. Train Accuracy: 67.395  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 2. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.627 Val. Acc: 68.104\n",
      "Epoch: 3. Train Loss: 0.629. Train Accuracy: 67.773  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.636 Val. Acc: 66.816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 6. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.635 Val. Acc: 67.359\n",
      "Epoch: 7. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 8. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.635 Val. Acc: 67.020\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 10. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 11. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.426\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 13. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.639 Val. Acc: 66.748\n",
      "Epoch: 14. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.698\n",
      "Epoch: 15. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 16. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 18. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.633 Val. Acc: 67.698\n",
      "Epoch: 19. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 20. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Test Loss: 0.645. Test Accuracy: 65.938\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.5900000000000002\n",
      "Epoch: 1. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.633 Val. Acc: 67.901\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 3. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.634 Val. Acc: 67.765\n",
      "Epoch: 4. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.632 Val. Acc: 67.630\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 67.901\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 7. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 8. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 10. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.631 Val. Acc: 67.901\n",
      "Epoch: 11. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.630\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 13. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 14. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.638 Val. Acc: 66.952\n",
      "Epoch: 15. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.633 Val. Acc: 67.630\n",
      "Epoch: 16. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Epoch: 17. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 18. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 19. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.629 Val. Acc: 67.969\n",
      "Epoch: 20. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Test Loss: 0.641. Test Accuracy: 66.142\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.6000000000000002\n",
      "Epoch: 1. Train Loss: 0.650. Train Accuracy: 66.394  Val. Loss: 0.646 Val. Acc: 67.698\n",
      "Epoch: 2. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.630 Val. Acc: 67.969\n",
      "Epoch: 3. Train Loss: 0.639. Train Accuracy: 66.260  Val. Loss: 0.646 Val. Acc: 67.833\n",
      "Epoch: 4. Train Loss: 0.633. Train Accuracy: 67.395  Val. Loss: 0.637 Val. Acc: 67.562\n",
      "Epoch: 5. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.639 Val. Acc: 67.901\n",
      "Epoch: 6. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 7. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.634 Val. Acc: 67.155\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.636 Val. Acc: 66.884\n",
      "Epoch: 10. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.638 Val. Acc: 67.020\n",
      "Epoch: 11. Train Loss: 0.612. Train Accuracy: 67.407  Val. Loss: 0.603 Val. Acc: 67.733\n",
      "Epoch: 12. Train Loss: 0.527. Train Accuracy: 67.029  Val. Loss: 0.558 Val. Acc: 67.869\n",
      "Epoch: 13. Train Loss: 0.415. Train Accuracy: 80.786  Val. Loss: 0.573 Val. Acc: 73.657\n",
      "Epoch: 14. Train Loss: 0.313. Train Accuracy: 91.138  Val. Loss: 0.543 Val. Acc: 75.455\n",
      "Epoch: 15. Train Loss: 0.242. Train Accuracy: 94.238  Val. Loss: 0.607 Val. Acc: 73.192\n",
      "Epoch: 16. Train Loss: 0.211. Train Accuracy: 95.007  Val. Loss: 0.568 Val. Acc: 75.352\n",
      "Epoch: 17. Train Loss: 0.163. Train Accuracy: 96.790  Val. Loss: 0.634 Val. Acc: 73.518\n",
      "Epoch: 18. Train Loss: 0.137. Train Accuracy: 97.290  Val. Loss: 0.669 Val. Acc: 73.812\n",
      "Epoch: 19. Train Loss: 0.123. Train Accuracy: 97.083  Val. Loss: 1.210 Val. Acc: 48.757\n",
      "Epoch: 20. Train Loss: 0.134. Train Accuracy: 96.423  Val. Loss: 0.737 Val. Acc: 73.515\n",
      "Test Loss: 0.786. Test Accuracy: 71.946\n",
      "Test: Recall: 0.72, Precision: 0.72, F-measure: 0.72\n",
      "\n",
      "Learning rate factor:  0.6100000000000002\n",
      "Epoch: 1. Train Loss: 0.639. Train Accuracy: 66.638  Val. Loss: 0.635 Val. Acc: 67.494\n",
      "Epoch: 2. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 3. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 5. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.639 Val. Acc: 67.359\n",
      "Epoch: 6. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.969\n",
      "Epoch: 7. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.631 Val. Acc: 67.969\n",
      "Epoch: 8. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.155\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.155\n",
      "Epoch: 10. Train Loss: 0.644. Train Accuracy: 65.515  Val. Loss: 0.638 Val. Acc: 67.426\n",
      "Epoch: 11. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 12. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.698\n",
      "Epoch: 13. Train Loss: 0.630. Train Accuracy: 67.786  Val. Loss: 0.630 Val. Acc: 67.833\n",
      "Epoch: 14. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 18. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.634 Val. Acc: 67.630\n",
      "Epoch: 19. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.634 Val. Acc: 67.698\n",
      "Epoch: 20. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Test Loss: 0.640. Test Accuracy: 66.277\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.6200000000000002\n",
      "Epoch: 1. Train Loss: 0.645. Train Accuracy: 67.017  Val. Loss: 0.637 Val. Acc: 67.562\n",
      "Epoch: 2. Train Loss: 0.643. Train Accuracy: 65.881  Val. Loss: 0.652 Val. Acc: 67.562\n",
      "Epoch: 3. Train Loss: 0.633. Train Accuracy: 67.395  Val. Loss: 0.639 Val. Acc: 67.698\n",
      "Epoch: 4. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.901\n",
      "Epoch: 5. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 7. Train Loss: 0.631. Train Accuracy: 67.395  Val. Loss: 0.645 Val. Acc: 67.562\n",
      "Epoch: 8. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 9. Train Loss: 0.630. Train Accuracy: 67.786  Val. Loss: 0.649 Val. Acc: 67.155\n",
      "Epoch: 10. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.627 Val. Acc: 67.969\n",
      "Epoch: 11. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.637 Val. Acc: 67.562\n",
      "Epoch: 12. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.636 Val. Acc: 66.816\n",
      "Epoch: 13. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.634 Val. Acc: 67.494\n",
      "Epoch: 14. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.635 Val. Acc: 67.562\n",
      "Epoch: 15. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.494\n",
      "Epoch: 16. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.654 Val. Acc: 66.613\n",
      "Epoch: 17. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 18. Train Loss: 0.633. Train Accuracy: 67.029  Val. Loss: 0.627 Val. Acc: 67.630\n",
      "Epoch: 19. Train Loss: 0.605. Train Accuracy: 67.407  Val. Loss: 0.612 Val. Acc: 67.359\n",
      "Epoch: 20. Train Loss: 0.525. Train Accuracy: 66.650  Val. Loss: 0.574 Val. Acc: 67.426\n",
      "Test Loss: 0.585. Test Accuracy: 66.277\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.6300000000000002\n",
      "Epoch: 1. Train Loss: 0.679. Train Accuracy: 62.402  Val. Loss: 0.676 Val. Acc: 67.155\n",
      "Epoch: 2. Train Loss: 0.643. Train Accuracy: 66.260  Val. Loss: 0.637 Val. Acc: 67.630\n",
      "Epoch: 3. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 4. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 5. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.640 Val. Acc: 67.969\n",
      "Epoch: 6. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.637 Val. Acc: 67.020\n",
      "Epoch: 7. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.638 Val. Acc: 67.223\n",
      "Epoch: 8. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.901\n",
      "Epoch: 9. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.660 Val. Acc: 67.698\n",
      "Epoch: 10. Train Loss: 0.636. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 11. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.664 Val. Acc: 67.494\n",
      "Epoch: 12. Train Loss: 0.642. Train Accuracy: 66.272  Val. Loss: 0.641 Val. Acc: 67.698\n",
      "Epoch: 13. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.636 Val. Acc: 67.087\n",
      "Epoch: 14. Train Loss: 0.630. Train Accuracy: 67.407  Val. Loss: 0.649 Val. Acc: 67.426\n",
      "Epoch: 15. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 16. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.642 Val. Acc: 67.630\n",
      "Epoch: 17. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.642 Val. Acc: 67.765\n",
      "Epoch: 18. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.901\n",
      "Epoch: 19. Train Loss: 0.607. Train Accuracy: 67.407  Val. Loss: 0.605 Val. Acc: 67.597\n",
      "Epoch: 20. Train Loss: 0.538. Train Accuracy: 67.407  Val. Loss: 0.626 Val. Acc: 67.326\n",
      "Test Loss: 0.656. Test Accuracy: 65.735\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.6400000000000002\n",
      "Epoch: 1. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.636 Val. Acc: 67.494\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 3. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 4. Train Loss: 0.643. Train Accuracy: 65.881  Val. Loss: 0.636 Val. Acc: 67.087\n",
      "Epoch: 5. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.635 Val. Acc: 67.291\n",
      "Epoch: 6. Train Loss: 0.643. Train Accuracy: 65.881  Val. Loss: 0.632 Val. Acc: 67.698\n",
      "Epoch: 7. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 8. Train Loss: 0.645. Train Accuracy: 65.503  Val. Loss: 0.636 Val. Acc: 67.291\n",
      "Epoch: 9. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 10. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 11. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.631 Val. Acc: 67.969\n",
      "Epoch: 12. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 13. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.634 Val. Acc: 67.494\n",
      "Epoch: 14. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.631 Val. Acc: 67.969\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 17. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 18. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.627 Val. Acc: 67.969\n",
      "Epoch: 19. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Test Loss: 0.644. Test Accuracy: 65.531\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.6500000000000002\n",
      "Epoch: 1. Train Loss: 0.644. Train Accuracy: 66.638  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 3. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.969\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.640 Val. Acc: 67.223\n",
      "Epoch: 5. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 6. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.645 Val. Acc: 67.630\n",
      "Epoch: 7. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.627 Val. Acc: 68.037\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 10. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.638 Val. Acc: 67.494\n",
      "Epoch: 11. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 68.037\n",
      "Epoch: 12. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.638 Val. Acc: 67.630\n",
      "Epoch: 13. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.639 Val. Acc: 67.291\n",
      "Epoch: 14. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.765\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.627 Val. Acc: 68.037\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 17. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.562\n",
      "Epoch: 18. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 19. Train Loss: 0.617. Train Accuracy: 66.919  Val. Loss: 0.607 Val. Acc: 71.023\n",
      "Epoch: 20. Train Loss: 0.461. Train Accuracy: 77.258  Val. Loss: 0.566 Val. Acc: 72.291\n",
      "Test Loss: 0.566. Test Accuracy: 72.679\n",
      "Test: Recall: 0.85, Precision: 0.73, F-measure: 0.77\n",
      "\n",
      "Learning rate factor:  0.6600000000000003\n",
      "Epoch: 1. Train Loss: 0.644. Train Accuracy: 67.017  Val. Loss: 0.636 Val. Acc: 67.223\n",
      "Epoch: 2. Train Loss: 0.633. Train Accuracy: 67.395  Val. Loss: 0.640 Val. Acc: 67.833\n",
      "Epoch: 3. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.635 Val. Acc: 67.291\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.630\n",
      "Epoch: 5. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.653 Val. Acc: 67.291\n",
      "Epoch: 6. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.628 Val. Acc: 68.240\n",
      "Epoch: 7. Train Loss: 0.629. Train Accuracy: 67.773  Val. Loss: 0.660 Val. Acc: 66.952\n",
      "Epoch: 8. Train Loss: 0.636. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 10. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.833\n",
      "Epoch: 11. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.643 Val. Acc: 67.359\n",
      "Epoch: 12. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.637 Val. Acc: 67.494\n",
      "Epoch: 13. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.638 Val. Acc: 67.562\n",
      "Epoch: 14. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 15. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.698\n",
      "Epoch: 16. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.640 Val. Acc: 67.223\n",
      "Epoch: 17. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 68.104\n",
      "Epoch: 18. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 20. Train Loss: 0.625. Train Accuracy: 67.407  Val. Loss: 0.629 Val. Acc: 67.698\n",
      "Test Loss: 0.646. Test Accuracy: 66.548\n",
      "Test: Recall: 1.00, Precision: 0.67, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.6700000000000003\n",
      "Epoch: 1. Train Loss: 0.635. Train Accuracy: 67.395  Val. Loss: 0.633 Val. Acc: 67.562\n",
      "Epoch: 2. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.639 Val. Acc: 67.833\n",
      "Epoch: 3. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 4. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 67.901\n",
      "Epoch: 5. Train Loss: 0.639. Train Accuracy: 66.260  Val. Loss: 0.636 Val. Acc: 67.223\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 8. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 10. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 11. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.636 Val. Acc: 66.952\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 13. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 14. Train Loss: 0.628. Train Accuracy: 67.786  Val. Loss: 0.643 Val. Acc: 67.155\n",
      "Epoch: 15. Train Loss: 0.636. Train Accuracy: 67.029  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 17. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.698\n",
      "Epoch: 18. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.640 Val. Acc: 67.291\n",
      "Epoch: 19. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 20. Train Loss: 0.636. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Test Loss: 0.639. Test Accuracy: 66.413\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.6800000000000003\n",
      "Epoch: 1. Train Loss: 0.642. Train Accuracy: 66.260  Val. Loss: 0.635 Val. Acc: 67.765\n",
      "Epoch: 2. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 3. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 8. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.635 Val. Acc: 67.087\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 10. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.634 Val. Acc: 67.630\n",
      "Epoch: 11. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 12. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.631 Val. Acc: 67.833\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.627 Val. Acc: 68.104\n",
      "Epoch: 14. Train Loss: 0.641. Train Accuracy: 65.894  Val. Loss: 0.664 Val. Acc: 67.155\n",
      "Epoch: 15. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.698\n",
      "Epoch: 16. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.833\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 18. Train Loss: 0.636. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 19. Train Loss: 0.600. Train Accuracy: 67.041  Val. Loss: 0.569 Val. Acc: 67.953\n",
      "Epoch: 20. Train Loss: 0.456. Train Accuracy: 77.039  Val. Loss: 0.616 Val. Acc: 69.396\n",
      "Test Loss: 0.629. Test Accuracy: 68.824\n",
      "Test: Recall: 0.69, Precision: 0.69, F-measure: 0.69\n",
      "\n",
      "Learning rate factor:  0.6900000000000003\n",
      "Epoch: 1. Train Loss: 0.639. Train Accuracy: 66.638  Val. Loss: 0.634 Val. Acc: 67.698\n",
      "Epoch: 2. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.641 Val. Acc: 66.884\n",
      "Epoch: 3. Train Loss: 0.643. Train Accuracy: 65.881  Val. Loss: 0.650 Val. Acc: 67.765\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.395  Val. Loss: 0.633 Val. Acc: 67.630\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.635 Val. Acc: 66.952\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.636 Val. Acc: 67.155\n",
      "Epoch: 7. Train Loss: 0.630. Train Accuracy: 67.786  Val. Loss: 0.633 Val. Acc: 67.698\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 9. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.637 Val. Acc: 67.494\n",
      "Epoch: 10. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 11. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 13. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 14. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.426\n",
      "Epoch: 15. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.494\n",
      "Epoch: 16. Train Loss: 0.614. Train Accuracy: 66.541  Val. Loss: 0.665 Val. Acc: 71.213\n",
      "Epoch: 17. Train Loss: 0.474. Train Accuracy: 76.416  Val. Loss: 0.616 Val. Acc: 71.317\n",
      "Epoch: 18. Train Loss: 0.323. Train Accuracy: 87.842  Val. Loss: 0.588 Val. Acc: 75.378\n",
      "Epoch: 19. Train Loss: 0.229. Train Accuracy: 91.980  Val. Loss: 0.602 Val. Acc: 75.849\n",
      "Epoch: 20. Train Loss: 0.163. Train Accuracy: 94.543  Val. Loss: 0.654 Val. Acc: 75.152\n",
      "Test Loss: 0.689. Test Accuracy: 73.283\n",
      "Test: Recall: 0.75, Precision: 0.73, F-measure: 0.74\n",
      "\n",
      "Learning rate factor:  0.7000000000000003\n",
      "Epoch: 1. Train Loss: 0.641. Train Accuracy: 66.260  Val. Loss: 0.639 Val. Acc: 67.698\n",
      "Epoch: 2. Train Loss: 0.646. Train Accuracy: 65.503  Val. Loss: 0.636 Val. Acc: 67.969\n",
      "Epoch: 3. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.633 Val. Acc: 67.155\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 5. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 7. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 9. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.359\n",
      "Epoch: 10. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 11. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.698\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 13. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 14. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.627 Val. Acc: 68.240\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.155\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 17. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.633 Val. Acc: 67.426\n",
      "Epoch: 18. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.627 Val. Acc: 68.104\n",
      "Epoch: 19. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.634 Val. Acc: 67.155\n",
      "Epoch: 20. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.633 Val. Acc: 67.494\n",
      "Test Loss: 0.643. Test Accuracy: 65.735\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.7100000000000003\n",
      "Epoch: 1. Train Loss: 0.640. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 2. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.638 Val. Acc: 67.359\n",
      "Epoch: 3. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 4. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Epoch: 5. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.833\n",
      "Epoch: 7. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.635 Val. Acc: 67.087\n",
      "Epoch: 8. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 9. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.562\n",
      "Epoch: 10. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.635 Val. Acc: 66.952\n",
      "Epoch: 11. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.638 Val. Acc: 67.291\n",
      "Epoch: 12. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.833\n",
      "Epoch: 13. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.626 Val. Acc: 68.172\n",
      "Epoch: 14. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 15. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.698\n",
      "Epoch: 16. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.627 Val. Acc: 68.037\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 18. Train Loss: 0.636. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 19. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 20. Train Loss: 0.636. Train Accuracy: 66.650  Val. Loss: 0.625 Val. Acc: 68.240\n",
      "Test Loss: 0.643. Test Accuracy: 65.735\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.7200000000000003\n",
      "Epoch: 1. Train Loss: 0.634. Train Accuracy: 67.395  Val. Loss: 0.635 Val. Acc: 67.223\n",
      "Epoch: 2. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 3. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.628 Val. Acc: 67.969\n",
      "Epoch: 4. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.635 Val. Acc: 66.952\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 8. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.632 Val. Acc: 67.765\n",
      "Epoch: 9. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.630\n",
      "Epoch: 10. Train Loss: 0.633. Train Accuracy: 67.407  Val. Loss: 0.638 Val. Acc: 67.359\n",
      "Epoch: 11. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.631 Val. Acc: 67.630\n",
      "Epoch: 12. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.633 Val. Acc: 67.494\n",
      "Epoch: 13. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 14. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 15. Train Loss: 0.638. Train Accuracy: 66.272  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 16. Train Loss: 0.627. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.291\n",
      "Epoch: 17. Train Loss: 0.554. Train Accuracy: 67.981  Val. Loss: 0.668 Val. Acc: 67.455\n",
      "Epoch: 18. Train Loss: 0.439. Train Accuracy: 71.118  Val. Loss: 0.688 Val. Acc: 66.903\n",
      "Epoch: 19. Train Loss: 0.343. Train Accuracy: 83.093  Val. Loss: 0.772 Val. Acc: 65.948\n",
      "Epoch: 20. Train Loss: 0.261. Train Accuracy: 90.222  Val. Loss: 0.784 Val. Acc: 65.596\n",
      "Test Loss: 0.802. Test Accuracy: 64.088\n",
      "Test: Recall: 0.67, Precision: 0.64, F-measure: 0.65\n",
      "\n",
      "Learning rate factor:  0.7300000000000003\n",
      "Epoch: 1. Train Loss: 0.636. Train Accuracy: 67.773  Val. Loss: 0.640 Val. Acc: 67.901\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 3. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.642 Val. Acc: 67.494\n",
      "Epoch: 4. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.636 Val. Acc: 67.494\n",
      "Epoch: 5. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.635 Val. Acc: 67.359\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 7. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.631 Val. Acc: 67.765\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 9. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.629 Val. Acc: 67.833\n",
      "Epoch: 10. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 11. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 12. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.638 Val. Acc: 67.630\n",
      "Epoch: 13. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.633 Val. Acc: 67.901\n",
      "Epoch: 14. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 15. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.636 Val. Acc: 67.020\n",
      "Epoch: 16. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.635 Val. Acc: 67.291\n",
      "Epoch: 17. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 18. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 68.037\n",
      "Epoch: 19. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Test Loss: 0.641. Test Accuracy: 66.006\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.7400000000000003\n",
      "Epoch: 1. Train Loss: 0.653. Train Accuracy: 65.503  Val. Loss: 0.659 Val. Acc: 67.630\n",
      "Epoch: 2. Train Loss: 0.642. Train Accuracy: 66.260  Val. Loss: 0.640 Val. Acc: 67.901\n",
      "Epoch: 3. Train Loss: 0.643. Train Accuracy: 65.881  Val. Loss: 0.644 Val. Acc: 67.969\n",
      "Epoch: 4. Train Loss: 0.636. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 6. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 8. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.634 Val. Acc: 67.426\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 10. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 11. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 13. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.638 Val. Acc: 67.494\n",
      "Epoch: 14. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.626 Val. Acc: 68.104\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 16. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.969\n",
      "Epoch: 17. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.638 Val. Acc: 67.494\n",
      "Epoch: 18. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 19. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Test Loss: 0.640. Test Accuracy: 66.209\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.80\n",
      "\n",
      "Learning rate factor:  0.7500000000000003\n",
      "Epoch: 1. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 3. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 4. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.639 Val. Acc: 67.901\n",
      "Epoch: 5. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.635 Val. Acc: 67.087\n",
      "Epoch: 6. Train Loss: 0.634. Train Accuracy: 67.017  Val. Loss: 0.634 Val. Acc: 67.020\n",
      "Epoch: 7. Train Loss: 0.631. Train Accuracy: 67.395  Val. Loss: 0.636 Val. Acc: 67.291\n",
      "Epoch: 8. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.698\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 10. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 12. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.426\n",
      "Epoch: 13. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 67.969\n",
      "Epoch: 14. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.645 Val. Acc: 67.291\n",
      "Epoch: 15. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 17. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 18. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 19. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Epoch: 20. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.765\n",
      "Test Loss: 0.646. Test Accuracy: 65.260\n",
      "Test: Recall: 1.00, Precision: 0.65, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.7600000000000003\n",
      "Epoch: 1. Train Loss: 0.640. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.426\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 3. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 4. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.637 Val. Acc: 67.291\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 6. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 67.901\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 67.901\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 10. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 11. Train Loss: 0.645. Train Accuracy: 65.515  Val. Loss: 0.639 Val. Acc: 67.291\n",
      "Epoch: 12. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 13. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.633 Val. Acc: 67.833\n",
      "Epoch: 14. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.698\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 16. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.630\n",
      "Epoch: 17. Train Loss: 0.630. Train Accuracy: 67.029  Val. Loss: 0.615 Val. Acc: 67.426\n",
      "Epoch: 18. Train Loss: 0.546. Train Accuracy: 66.333  Val. Loss: 0.576 Val. Acc: 72.446\n",
      "Epoch: 19. Train Loss: 0.398. Train Accuracy: 83.862  Val. Loss: 0.584 Val. Acc: 71.759\n",
      "Epoch: 20. Train Loss: 0.305. Train Accuracy: 90.100  Val. Loss: 0.673 Val. Acc: 67.330\n",
      "Test Loss: 0.717. Test Accuracy: 65.912\n",
      "Test: Recall: 0.65, Precision: 0.66, F-measure: 0.65\n",
      "\n",
      "Learning rate factor:  0.7700000000000004\n",
      "Epoch: 1. Train Loss: 0.642. Train Accuracy: 66.260  Val. Loss: 0.639 Val. Acc: 67.223\n",
      "Epoch: 2. Train Loss: 0.640. Train Accuracy: 66.260  Val. Loss: 0.637 Val. Acc: 67.698\n",
      "Epoch: 3. Train Loss: 0.642. Train Accuracy: 65.881  Val. Loss: 0.640 Val. Acc: 67.969\n",
      "Epoch: 4. Train Loss: 0.633. Train Accuracy: 67.407  Val. Loss: 0.639 Val. Acc: 67.630\n",
      "Epoch: 5. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.642 Val. Acc: 67.833\n",
      "Epoch: 6. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.637 Val. Acc: 67.901\n",
      "Epoch: 7. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.223\n",
      "Epoch: 8. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.633 Val. Acc: 67.494\n",
      "Epoch: 9. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 10. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 11. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.632 Val. Acc: 67.291\n",
      "Epoch: 12. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.636 Val. Acc: 67.087\n",
      "Epoch: 13. Train Loss: 0.622. Train Accuracy: 67.407  Val. Loss: 0.628 Val. Acc: 67.833\n",
      "Epoch: 14. Train Loss: 0.531. Train Accuracy: 67.273  Val. Loss: 0.542 Val. Acc: 67.536\n",
      "Epoch: 15. Train Loss: 0.376. Train Accuracy: 84.937  Val. Loss: 0.633 Val. Acc: 73.741\n",
      "Epoch: 16. Train Loss: 0.267. Train Accuracy: 91.309  Val. Loss: 0.636 Val. Acc: 72.249\n",
      "Epoch: 17. Train Loss: 0.194. Train Accuracy: 94.531  Val. Loss: 0.715 Val. Acc: 71.036\n",
      "Epoch: 18. Train Loss: 0.139. Train Accuracy: 96.545  Val. Loss: 0.762 Val. Acc: 74.199\n",
      "Epoch: 19. Train Loss: 0.111. Train Accuracy: 97.473  Val. Loss: 0.793 Val. Acc: 74.642\n",
      "Epoch: 20. Train Loss: 0.096. Train Accuracy: 97.900  Val. Loss: 0.861 Val. Acc: 72.085\n",
      "Test Loss: 0.931. Test Accuracy: 69.651\n",
      "Test: Recall: 0.70, Precision: 0.70, F-measure: 0.70\n",
      "\n",
      "Learning rate factor:  0.7800000000000004\n",
      "Epoch: 1. Train Loss: 0.640. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.426\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.629 Val. Acc: 67.698\n",
      "Epoch: 3. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.634 Val. Acc: 67.291\n",
      "Epoch: 4. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 5. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.638 Val. Acc: 67.087\n",
      "Epoch: 6. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 7. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 8. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 9. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.632 Val. Acc: 67.494\n",
      "Epoch: 10. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 11. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 12. Train Loss: 0.642. Train Accuracy: 65.894  Val. Loss: 0.638 Val. Acc: 67.562\n",
      "Epoch: 13. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.635 Val. Acc: 67.155\n",
      "Epoch: 14. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.635 Val. Acc: 66.884\n",
      "Epoch: 15. Train Loss: 0.639. Train Accuracy: 66.272  Val. Loss: 0.632 Val. Acc: 67.833\n",
      "Epoch: 16. Train Loss: 0.647. Train Accuracy: 65.137  Val. Loss: 0.639 Val. Acc: 67.833\n",
      "Epoch: 17. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.634 Val. Acc: 67.291\n",
      "Epoch: 18. Train Loss: 0.629. Train Accuracy: 67.786  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 19. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.636 Val. Acc: 67.087\n",
      "Epoch: 20. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.223\n",
      "Test Loss: 0.645. Test Accuracy: 65.938\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n",
      "Learning rate factor:  0.7900000000000004\n",
      "Epoch: 1. Train Loss: 0.639. Train Accuracy: 66.638  Val. Loss: 0.631 Val. Acc: 67.698\n",
      "Epoch: 2. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.646 Val. Acc: 67.426\n",
      "Epoch: 3. Train Loss: 0.633. Train Accuracy: 67.395  Val. Loss: 0.638 Val. Acc: 67.223\n",
      "Epoch: 4. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.633 Val. Acc: 67.765\n",
      "Epoch: 5. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.636 Val. Acc: 67.833\n",
      "Epoch: 6. Train Loss: 0.631. Train Accuracy: 67.407  Val. Loss: 0.639 Val. Acc: 67.833\n",
      "Epoch: 7. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.639 Val. Acc: 67.155\n",
      "Epoch: 8. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.636 Val. Acc: 67.562\n",
      "Epoch: 9. Train Loss: 0.638. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 10. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 11. Train Loss: 0.636. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.833\n",
      "Epoch: 12. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 14. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.630\n",
      "Epoch: 15. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.632 Val. Acc: 67.630\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.630\n",
      "Epoch: 17. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.630 Val. Acc: 67.698\n",
      "Epoch: 18. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.637 Val. Acc: 67.020\n",
      "Epoch: 19. Train Loss: 0.640. Train Accuracy: 66.272  Val. Loss: 0.633 Val. Acc: 67.630\n",
      "Epoch: 20. Train Loss: 0.629. Train Accuracy: 66.650  Val. Loss: 0.602 Val. Acc: 67.901\n",
      "Test Loss: 0.619. Test Accuracy: 65.667\n",
      "Test: Recall: 1.00, Precision: 0.66, F-measure: 0.79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix the seeds to get consistent results\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "INPUT_DIM = len(word2idx_a)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "#the hyperparamerts specific to CNN\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 1\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "lr = np.arange(0.4, 0.8, 0.01)\n",
    "test_acc = []\n",
    "test_f1 = []\n",
    "\n",
    "for l in lr:\n",
    "    print('Learning rate factor: ', l)\n",
    "    mymodel = MyModel(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS).to('cuda')\n",
    "    mymodel.apply(init_weights)\n",
    "    mymodel.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "    \n",
    "    optimizer = optim.RMSprop(mymodel.parameters(), lr=l*3e-3,weight_decay=1e-7)\n",
    "    \n",
    "    acc, f1 = train_model(mymodel, train_a_loader, valid_a_loader, test_a_loader)\n",
    "    test_acc.append(acc)\n",
    "    test_f1.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6900000000000003\n",
      "accuracy 0.7328254147009416\n",
      "f1 0.7390093180753137\n",
      "0.45000000000000007\n",
      "accuracy 0.6675167896530845\n",
      "f1 0.799767124967593\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21b00415048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_acc = np.argmax(test_acc)\n",
    "print(lr[best_acc])\n",
    "print('accuracy', test_acc[best_acc])\n",
    "print('f1', test_f1[best_acc])\n",
    "\n",
    "best_f1 = np.argmax(test_f1)\n",
    "print(lr[best_f1])\n",
    "print('accuracy', test_acc[best_f1])\n",
    "print('f1', test_f1[best_f1])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "#ax1.title('Test results over learning rate')\n",
    "\n",
    "ax1.plot(lr, test_acc, color='r')\n",
    "ax1.set_ylabel('Test accuracy', color='r')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(lr, test_f1, color='b')\n",
    "ax2.set_ylabel('Test f1 score', color='b')\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Train Loss: 0.641. Train Accuracy: 67.017  Val. Loss: 0.627 Val. Acc: 68.037\n",
      "Epoch: 2. Train Loss: 0.635. Train Accuracy: 67.017  Val. Loss: 0.632 Val. Acc: 67.630\n",
      "Epoch: 3. Train Loss: 0.632. Train Accuracy: 67.395  Val. Loss: 0.647 Val. Acc: 67.155\n",
      "Epoch: 4. Train Loss: 0.638. Train Accuracy: 66.638  Val. Loss: 0.632 Val. Acc: 67.359\n",
      "Epoch: 5. Train Loss: 0.637. Train Accuracy: 66.638  Val. Loss: 0.629 Val. Acc: 67.901\n",
      "Epoch: 6. Train Loss: 0.635. Train Accuracy: 67.029  Val. Loss: 0.628 Val. Acc: 67.901\n",
      "Epoch: 7. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.634 Val. Acc: 67.698\n",
      "Epoch: 8. Train Loss: 0.643. Train Accuracy: 65.894  Val. Loss: 0.640 Val. Acc: 67.291\n",
      "Epoch: 9. Train Loss: 0.632. Train Accuracy: 67.407  Val. Loss: 0.627 Val. Acc: 68.104\n",
      "Epoch: 10. Train Loss: 0.630. Train Accuracy: 67.786  Val. Loss: 0.632 Val. Acc: 67.562\n",
      "Epoch: 11. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.626 Val. Acc: 68.172\n",
      "Epoch: 12. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.629 Val. Acc: 67.765\n",
      "Epoch: 13. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.633 Val. Acc: 67.291\n",
      "Epoch: 14. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.631 Val. Acc: 67.494\n",
      "Epoch: 15. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.634 Val. Acc: 67.087\n",
      "Epoch: 16. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.630 Val. Acc: 67.562\n",
      "Epoch: 17. Train Loss: 0.634. Train Accuracy: 67.029  Val. Loss: 0.633 Val. Acc: 67.359\n",
      "Epoch: 18. Train Loss: 0.637. Train Accuracy: 66.650  Val. Loss: 0.631 Val. Acc: 67.562\n",
      "Epoch: 19. Train Loss: 0.622. Train Accuracy: 67.029  Val. Loss: 0.598 Val. Acc: 67.359\n",
      "Epoch: 20. Train Loss: 0.494. Train Accuracy: 69.995  Val. Loss: 0.563 Val. Acc: 73.828\n",
      "Test Loss: 0.575. Test Accuracy: 73.218\n",
      "Test: Recall: 0.85, Precision: 0.73, F-measure: 0.77\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7321797555143182, 0.7661534267592857)"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix the seeds to get consistent results\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "INPUT_DIM = len(word2idx_a)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "#the hyperparamerts specific to CNN\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 1\n",
    "# we define the window size\n",
    "WINDOW_SIZE = 1\n",
    "mymodel_a = MyModel(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS).to('cuda')\n",
    "mymodel_a.apply(init_weights)\n",
    "mymodel_a.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "\n",
    "optimizer = optim.RMSprop(mymodel_a.parameters(), lr=0.69*3e-3,weight_decay=1e-7)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_model(mymodel_a, train_a_loader, valid_a_loader, test_a_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Train Loss: 0.694. Train Accuracy: 49.837  Val. Loss: 0.693 Val. Acc: 48.422\n",
      "Epoch: 2. Train Loss: 0.694. Train Accuracy: 49.878  Val. Loss: 0.693 Val. Acc: 49.442\n",
      "Epoch: 3. Train Loss: 0.694. Train Accuracy: 50.347  Val. Loss: 0.695 Val. Acc: 49.442\n",
      "Epoch: 4. Train Loss: 0.694. Train Accuracy: 49.736  Val. Loss: 0.693 Val. Acc: 49.609\n",
      "Epoch: 5. Train Loss: 0.694. Train Accuracy: 49.981  Val. Loss: 0.695 Val. Acc: 50.574\n",
      "Epoch: 6. Train Loss: 0.693. Train Accuracy: 50.286  Val. Loss: 0.693 Val. Acc: 50.574\n",
      "Epoch: 7. Train Loss: 0.693. Train Accuracy: 49.777  Val. Loss: 0.691 Val. Acc: 46.716\n",
      "Epoch: 8. Train Loss: 0.693. Train Accuracy: 50.388  Val. Loss: 0.692 Val. Acc: 50.574\n",
      "Epoch: 9. Train Loss: 0.693. Train Accuracy: 49.981  Val. Loss: 0.693 Val. Acc: 48.645\n",
      "Epoch: 10. Train Loss: 0.681. Train Accuracy: 50.697  Val. Loss: 0.656 Val. Acc: 61.878\n",
      "Epoch: 11. Train Loss: 0.588. Train Accuracy: 70.161  Val. Loss: 0.621 Val. Acc: 66.661\n",
      "Epoch: 12. Train Loss: 0.519. Train Accuracy: 73.634  Val. Loss: 0.566 Val. Acc: 68.447\n",
      "Epoch: 13. Train Loss: 0.388. Train Accuracy: 78.748  Val. Loss: 0.384 Val. Acc: 88.783\n",
      "Epoch: 14. Train Loss: 0.208. Train Accuracy: 95.189  Val. Loss: 0.283 Val. Acc: 90.370\n",
      "Epoch: 15. Train Loss: 0.116. Train Accuracy: 97.821  Val. Loss: 0.254 Val. Acc: 91.948\n",
      "Epoch: 16. Train Loss: 0.077. Train Accuracy: 98.110  Val. Loss: 0.226 Val. Acc: 93.638\n",
      "Epoch: 17. Train Loss: 0.056. Train Accuracy: 98.520  Val. Loss: 0.248 Val. Acc: 92.881\n",
      "Epoch: 18. Train Loss: 0.043. Train Accuracy: 98.972  Val. Loss: 0.285 Val. Acc: 93.383\n",
      "Epoch: 19. Train Loss: 0.033. Train Accuracy: 99.239  Val. Loss: 0.189 Val. Acc: 95.033\n",
      "Epoch: 20. Train Loss: 0.028. Train Accuracy: 99.384  Val. Loss: 0.251 Val. Acc: 92.642\n",
      "Test Loss: 0.172. Test Accuracy: 95.815\n",
      "Test: Recall: 0.96, Precision: 0.96, F-measure: 0.96\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9581473214285714, 0.9582222874625943)"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix the seeds to get consistent results\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "INPUT_DIM = len(word2idx_a)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "#the hyperparamerts specific to CNN\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 1\n",
    "# we define the window size\n",
    "WINDOW_SIZE = 1\n",
    "mymodel_b = MyModel(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS).to('cuda')\n",
    "mymodel_b.apply(init_weights)\n",
    "mymodel_b.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "\n",
    "optimizer = optim.Adam(mymodel_b.parameters(), lr=689e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_model(mymodel_b, train_b_loader, valid_b_loader, test_b_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    " def accuracy_c(output, target):\n",
    "    correct = torch.sum(torch.argmax(output, dim = 1).float() == target.float(), dtype = torch.float)\n",
    "    acc = correct / len(target)\n",
    " \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure_c(output, gold):\n",
    "\n",
    "    output = torch.argmax(output, dim = 1)\n",
    "    output = output.cpu().numpy()\n",
    "    gold = gold.cpu().numpy()\n",
    "\n",
    "    recall, precision, fscore, _ = precision_recall_fscore_support(output, gold, labels=[0, 1, 2])\n",
    "    recall_m, precision_m, fscore_m, _ = precision_recall_fscore_support(output, gold, labels=[0, 1, 2], average='macro')\n",
    "    \n",
    "    return np.append(recall, recall_m), np.append(precision, precision_m), np.append(fscore, fscore_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_c(model, train_loader, valid_loader, test_loader):  \n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        model.train()\n",
    "        \n",
    "        recall = 0\n",
    "        precision = 0\n",
    "        fscore = 0\n",
    "        \n",
    "        count_train = 0\n",
    "        count_valid = 0\n",
    "        count_test = 0\n",
    "       \n",
    "        #iterate over batches\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            #place on the GPU\n",
    "            #feature, target = batch.text.cuda(), batch.label.cuda()\n",
    "            \n",
    "            feature, target = batch\n",
    "            \n",
    "            feature = feature.to('cuda')\n",
    "            target = target.to('cuda')\n",
    " \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(feature).squeeze(1)\n",
    "             \n",
    "            loss = loss_fn(predictions, target.long())\n",
    "            acc = accuracy_c(predictions, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            count_train += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_valid in valid_loader:\n",
    "                feature_valid, target_valid = batch_valid\n",
    "                \n",
    "                feature_valid = feature_valid.to('cuda')\n",
    "                target_valid = target_valid.to('cuda')\n",
    "                \n",
    "                predictions_valid = model(feature_valid).squeeze(1)\n",
    "                loss = loss_fn(predictions_valid, target_valid.long())\n",
    "                acc = accuracy_c(predictions_valid, target_valid)\n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += acc.item()\n",
    "                count_valid += 1\n",
    "            \n",
    "        valid_loss = valid_loss /count_valid\n",
    "        valid_acc = valid_acc /count_valid\n",
    "        epoch_loss = epoch_loss /count_train\n",
    "        epoch_acc = epoch_acc /count_train\n",
    "        \n",
    "        print('Epoch: {}. Train Loss: {:.3f}. Train Accuracy: {:.3f}  Val. Loss: {:.3f} Val. Acc: {:.3f}'.format(epoch, epoch_loss, epoch_acc*100, valid_loss, valid_acc*100))\n",
    "        \n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_test in test_loader:\n",
    "            feature_test, target_test = batch_test\n",
    "            \n",
    "            feature_test = feature_test.to('cuda')\n",
    "            target_test = target_test.to('cuda')\n",
    "            \n",
    "            predictions_test = model(feature_test).squeeze(1)\n",
    "            loss = loss_fn(predictions_test, target_test.long())\n",
    "            acc = accuracy_c(predictions_test, target_test)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += acc.item()\n",
    "            \n",
    "            r, p, f = f_measure_c(predictions_test, target_test)\n",
    "            recall += r\n",
    "            precision += p\n",
    "            fscore += f\n",
    "            count_test += 1\n",
    "\n",
    "    test_loss = test_loss /count_test\n",
    "    test_acc = test_acc /count_test\n",
    "    recall = recall /count_test\n",
    "    precision = precision /count_test\n",
    "    fscore = fscore /count_test\n",
    "\n",
    "    print('Test Loss: {:.3f}. Test Accuracy: {:.3f}'.format(test_loss, test_acc*100))\n",
    "    print(\"Class 0: Recall: %.2f, Precision: %.2f, F-measure: %.2f\\n\" % (recall[0], precision[0], fscore[0]))\n",
    "    print(\"Class 1: Recall: %.2f, Precision: %.2f, F-measure: %.2f\\n\" % (recall[1], precision[1], fscore[1]))\n",
    "    print(\"Class 2: Recall: %.2f, Precision: %.2f, F-measure: %.2f\\n\" % (recall[2], precision[2], fscore[2]))\n",
    "    print(\"Macro: Recall: %.2f, Precision: %.2f, F-measure: %.2f\\n\" % (recall[3], precision[3], fscore[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Train Loss: 1.157. Train Accuracy: 41.037  Val. Loss: 0.957 Val. Acc: 50.985\n",
      "Epoch: 2. Train Loss: 0.763. Train Accuracy: 69.011  Val. Loss: 0.598 Val. Acc: 76.961\n",
      "Epoch: 3. Train Loss: 0.391. Train Accuracy: 87.120  Val. Loss: 0.459 Val. Acc: 82.644\n",
      "Epoch: 4. Train Loss: 0.198. Train Accuracy: 94.671  Val. Loss: 0.410 Val. Acc: 84.342\n",
      "Epoch: 5. Train Loss: 0.099. Train Accuracy: 97.717  Val. Loss: 0.404 Val. Acc: 85.647\n",
      "Epoch: 6. Train Loss: 0.057. Train Accuracy: 98.667  Val. Loss: 0.386 Val. Acc: 87.046\n",
      "Epoch: 7. Train Loss: 0.037. Train Accuracy: 99.234  Val. Loss: 0.438 Val. Acc: 85.472\n",
      "Epoch: 8. Train Loss: 0.027. Train Accuracy: 99.472  Val. Loss: 0.422 Val. Acc: 87.001\n",
      "Epoch: 9. Train Loss: 0.022. Train Accuracy: 99.581  Val. Loss: 0.456 Val. Acc: 86.103\n",
      "Epoch: 10. Train Loss: 0.020. Train Accuracy: 99.586  Val. Loss: 0.434 Val. Acc: 85.585\n",
      "Epoch: 11. Train Loss: 0.015. Train Accuracy: 99.768  Val. Loss: 0.482 Val. Acc: 86.119\n",
      "Epoch: 12. Train Loss: 0.013. Train Accuracy: 99.701  Val. Loss: 0.482 Val. Acc: 86.103\n",
      "Epoch: 13. Train Loss: 0.011. Train Accuracy: 99.862  Val. Loss: 0.495 Val. Acc: 86.066\n",
      "Epoch: 14. Train Loss: 0.009. Train Accuracy: 99.839  Val. Loss: 0.518 Val. Acc: 85.277\n",
      "Epoch: 15. Train Loss: 0.007. Train Accuracy: 99.862  Val. Loss: 0.506 Val. Acc: 86.220\n",
      "Epoch: 16. Train Loss: 0.007. Train Accuracy: 99.883  Val. Loss: 0.537 Val. Acc: 85.899\n",
      "Epoch: 17. Train Loss: 0.006. Train Accuracy: 99.862  Val. Loss: 0.546 Val. Acc: 85.720\n",
      "Epoch: 18. Train Loss: 0.005. Train Accuracy: 99.862  Val. Loss: 0.545 Val. Acc: 85.647\n",
      "Epoch: 19. Train Loss: 0.005. Train Accuracy: 99.862  Val. Loss: 0.558 Val. Acc: 85.915\n",
      "Epoch: 20. Train Loss: 0.004. Train Accuracy: 99.906  Val. Loss: 0.553 Val. Acc: 86.139\n",
      "Test Loss: 0.525. Test Accuracy: 86.708\n",
      "Class 0: Recall: 0.84, Precision: 0.84, F-measure: 0.84\n",
      "\n",
      "Class 1: Recall: 0.78, Precision: 0.85, F-measure: 0.81\n",
      "\n",
      "Class 2: Recall: 0.98, Precision: 0.91, F-measure: 0.95\n",
      "\n",
      "Macro: Recall: 0.87, Precision: 0.87, F-measure: 0.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix the seeds to get consistent results\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Train method, not adapted yet\n",
    "\n",
    "INPUT_DIM = len(word2idx_a)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 3\n",
    "#the hyperparamerts specific to CNN\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 3\n",
    "# we define the window size\n",
    "WINDOW_SIZE = 1\n",
    "mymodel_c = MyModel(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS).to('cuda')\n",
    "mymodel_c.apply(init_weights)\n",
    "mymodel_c.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "\n",
    "optimizer = optim.RMSprop(mymodel_c.parameters(), lr = 1138e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model_c(mymodel_c, train_c_loader, valid_c_loader, test_c_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions of test data for submission\n",
    "predictions_eva_a = []\n",
    "predictions_eva_b = []\n",
    "predictions_eva_c = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    feature_eva_a = eva_data_a.to('cuda')\n",
    "    predictions_eva_a.append(mymodel_a(feature_eva_a))\n",
    "    \n",
    "    feature_eva_b = eva_data_b.to('cuda')\n",
    "    predictions_eva_b.append(mymodel_b(feature_eva_b))\n",
    "    \n",
    "    feature_eva_c = eva_data_c.to('cuda')\n",
    "    predictions_eva_c.append(mymodel_c(feature_eva_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results_a = predictions_eva_a[0].cpu().numpy().flatten()\n",
    "predict_results_b = predictions_eva_b[0].cpu().numpy().flatten()\n",
    "predict_results_c = predictions_eva_c[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels and write csv\n",
    "predict_label_a = np.empty(len(predict_results_a), dtype = object)\n",
    "\n",
    "predict_label_a[predict_results_a < 0.5] = \"NOT\"\n",
    "predict_label_a[predict_results_a >= 0.5] = \"OFF\"\n",
    "\n",
    "df_a = pd.DataFrame({'Id':eva_id_a, 'Label':predict_label_a})\n",
    "df_a.to_csv('task_a_results.csv', index=False, header = False)\n",
    "\n",
    "predict_label_b = np.empty(len(predict_results_b), dtype = object)\n",
    "\n",
    "predict_label_b[predict_results_b < 0.5] = \"TIN\"\n",
    "predict_label_b[predict_results_b >= 0.5] = \"UNT\"\n",
    "\n",
    "df_b = pd.DataFrame({'Id':eva_id_b, 'Label':predict_label_b})\n",
    "df_b.to_csv('task_b_results.csv', index=False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_label_c = np.empty(len(predict_results_c), dtype = object)\n",
    "\n",
    "predict_label_c[np.argmax(predict_results_c, axis = 1) == 0] = \"GRP\"\n",
    "predict_label_c[np.argmax(predict_results_c, axis = 1) == 1] = \"IND\"\n",
    "predict_label_c[np.argmax(predict_results_c, axis = 1) == 2] = \"OTH\"\n",
    "\n",
    "df_c = pd.DataFrame({'Id':eva_id_c, 'Label':predict_label_c})\n",
    "df_c.to_csv('task_c_results.csv', index=False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
